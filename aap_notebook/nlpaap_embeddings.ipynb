{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3h0ffPu0LOM",
        "outputId": "f019c91d-8dc0-4294-fb1d-fa6da98ce54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHXcBgfQXWjk"
      },
      "source": [
        "I'm going to use 3 ways for embeddings\n",
        "- count vectorizer\n",
        "- tfidf\n",
        "- word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kBVfdNnbQ4M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqGuG-2fXrXm"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/My Drive/train_clean.csv'\n",
        "test_path = '/content/drive/My Drive/test_clean.csv'\n",
        "train_data = pd.read_csv(train_path,sep = \",\", header=0)\n",
        "test_data = pd.read_csv(test_path, sep = \",\", header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtB7997IcBP3"
      },
      "outputs": [],
      "source": [
        "train = train_data[\"preprocessed_text\"]\n",
        "train_target = train_data[\"type\"]\n",
        "\n",
        "test = test_data[\"preprocessed_text\"]\n",
        "test_target = test_data[\"type\"]\n",
        "\n",
        "# Create a new DataFrame for the train data with preprocessed text and target\n",
        "train_combined = pd.DataFrame({\n",
        "    'preprocessed_text': train_data[\"preprocessed_text\"],\n",
        "    'target': train_data[\"type\"]\n",
        "})\n",
        "\n",
        "# Create a new DataFrame for the test data with preprocessed text and target\n",
        "test_combined = pd.DataFrame({\n",
        "    'preprocessed_text': test_data[\"preprocessed_text\"],\n",
        "    'target': test_data[\"type\"]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcVsF1t9lcdX",
        "outputId": "228deb35-2d2e-4a28-e81a-fcce8e3d0c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned Train Data (Rows with missing data removed):\n",
            "              preprocessed_text    target\n",
            "0  im getting borderland murder  Positive\n",
            "1            coming border kill  Positive\n",
            "2    im getting borderland kill  Positive\n",
            "3   im coming borderland murder  Positive\n",
            "4  im getting borderland murder  Positive\n",
            "\n",
            "Missing Data Rate for each column:\n",
            "preprocessed_text    2.269175\n",
            "target               0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# deal with missing data and show the missing data rate\n",
        "\n",
        "def remove_missing_data_and_calculate_rate(df):\n",
        "    # Step 1: Calculate the missing data rate\n",
        "    missing_data_rate = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    # Step 2: Drop rows with missing data\n",
        "    df_cleaned = df.dropna()\n",
        "\n",
        "    return df_cleaned, missing_data_rate\n",
        "\n",
        "train_cleaned, missing_data_rate = remove_missing_data_and_calculate_rate(train_combined)\n",
        "\n",
        "\n",
        "print(\"Cleaned Train Data (Rows with missing data removed):\")\n",
        "print(train_cleaned.head())\n",
        "\n",
        "print(\"\\nMissing Data Rate for each column:\")\n",
        "print(missing_data_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JE5-h-SnbtP",
        "outputId": "b7bfa2b4-751c-4710-ec82-bb0b295f6837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned Test Data (Rows with missing data removed):\n",
            "              preprocessed_text    target\n",
            "0  im getting borderland murder  Positive\n",
            "1            coming border kill  Positive\n",
            "2    im getting borderland kill  Positive\n",
            "3   im coming borderland murder  Positive\n",
            "4  im getting borderland murder  Positive\n",
            "\n",
            "Missing Data Rate for each column:\n",
            "preprocessed_text    0.1\n",
            "target               0.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "test_cleaned, missing_data_rate = remove_missing_data_and_calculate_rate(test_combined)\n",
        "print(\"Cleaned Test Data (Rows with missing data removed):\")\n",
        "print(train_cleaned.head())\n",
        "\n",
        "print(\"\\nMissing Data Rate for each column:\")\n",
        "print(missing_data_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaUDpTA3d2v6",
        "outputId": "1f8702b3-088b-4fdc-cf8d-591184496d7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlc4s-T-hYZu",
        "outputId": "a6264d51-3cbe-4f90-fe82-93c90cf11867"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "71656"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHqm8gFpe8P"
      },
      "outputs": [],
      "source": [
        "train = train_cleaned[\"preprocessed_text\"]\n",
        "train_target = train_cleaned[\"target\"]\n",
        "test = test_cleaned[\"preprocessed_text\"]\n",
        "test_target = test_cleaned[\"target\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8UvlxlVcvld"
      },
      "source": [
        "1. count vecorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCLxlCGJpT2M"
      },
      "outputs": [],
      "source": [
        "def count_vectorizer(train, test = None):\n",
        "    cv = CountVectorizer()\n",
        "    train = cv.fit_transform(train)\n",
        "    if test is not None:\n",
        "        test = cv.transform(test)\n",
        "        return train, test, cv\n",
        "    else:\n",
        "        return train, cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX2vDdcmqE0A"
      },
      "outputs": [],
      "source": [
        "train_cv , test_cv, cv = count_vectorizer(train,test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HKHTeWoqOSu",
        "outputId": "5cf2e10a-6297-4c2e-946c-656b5456ec79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['aa', 'aaa', 'aaaaaaaaaaaa', ..., 'zysola', 'zzvfsrhewg', 'zzz'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPQwC9DfqTaP",
        "outputId": "ccad623b-ff0e-4617-b1b2-a6e185d149a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_cv.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1dOaqBNdqzH"
      },
      "source": [
        "2. tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fApFNd_dsNf"
      },
      "outputs": [],
      "source": [
        "def tfidf_vectorizer(train, test = None):\n",
        "    tfidf = TfidfVectorizer(stop_words = \"english\", ngram_range = (1,2), analyzer = \"word\",\n",
        "                            max_df = 0.5, binary = False,  token_pattern=r'\\w+', sublinear_tf=False\n",
        "                           )\n",
        "\n",
        "    # max_df This parameter sets the maximum document frequency threshold.\n",
        "    # It means that if a word appears in more than 50% of the documents,\n",
        "    # it will be ignored because it is considered too common to be useful\n",
        "    # for distinguishing between documents.\n",
        "    train = tfidf.fit_transform(train)\n",
        "    if test is not None:\n",
        "        test = tfidf.transform(test)\n",
        "        return train, test, tfidf\n",
        "    else:\n",
        "        return train, tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-BeiDdlqrQT"
      },
      "outputs": [],
      "source": [
        "train_tfidf, test_tfidf, tfidf = tfidf_vectorizer(train,test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rilaTB47dwWf"
      },
      "source": [
        "3. word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjFZtPBqdzn4"
      },
      "outputs": [],
      "source": [
        "# Function to train Word2Vec and transform data into embeddings\n",
        "def word2vec(train, test=None):\n",
        "    # Step 1: Train Word2Vec model on training data\n",
        "    word2vec_model = Word2Vec(sentences=[i.split() for i in train], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "    # Step 2: Function to generate sentence vectors by averaging word vectors\n",
        "    def get_sentence_vector(sentence, model, vector_size):\n",
        "        sentence_vector = np.zeros(vector_size)  # Initialize an empty vector\n",
        "        count = 0\n",
        "        for word in sentence.split():\n",
        "            if word in model.wv:\n",
        "                sentence_vector += model.wv[word]\n",
        "                count += 1\n",
        "        if count != 0:\n",
        "            sentence_vector /= count  # Average the vectors\n",
        "        return sentence_vector\n",
        "\n",
        "    # Step 3: Convert train dataset to sentence vectors\n",
        "    train_vectors = np.array([get_sentence_vector(sentence, word2vec_model, 100) for sentence in train])\n",
        "\n",
        "    # If test data is provided, apply the same transformation\n",
        "    if test is not None:\n",
        "        test_vectors = np.array([get_sentence_vector(sentence, word2vec_model, 100) for sentence in test])\n",
        "        return train_vectors, test_vectors, word2vec_model\n",
        "    else:\n",
        "        return train_vectors, word2vec_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he1b0Upvq2d4"
      },
      "outputs": [],
      "source": [
        "train_word2vec, test_word2vec, word2vec = word2vec(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V61NGoZvq3vc",
        "outputId": "5e0bbb77-623a-4243-97a7-d9515a91f110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector for 'pubg':\n",
            "[-0.01912192  0.8862802  -0.09390827 -2.6960688  -0.26106402 -0.09441577\n",
            " -0.53028303 -0.5510987  -1.0323496  -0.67864054 -0.20726399 -0.12606059\n",
            " -2.18844    -0.45130172  0.11052801  0.10869858 -0.09418953 -1.0169723\n",
            "  1.2788637  -2.8027942   1.4971087   0.2049509   1.5463896  -0.076642\n",
            " -0.39039338  0.26753086  1.3301667  -1.0254695  -0.68401706 -0.01117503\n",
            "  1.6573915  -0.09380846 -0.5970899  -2.5046206  -0.19524558 -0.45864773\n",
            " -1.1758468  -0.33340088  0.88980734  0.35717857 -0.8066866  -1.1878077\n",
            " -0.3714643  -1.1475065  -0.8210059  -0.07598607 -0.1593263  -1.8985881\n",
            "  0.17395508 -0.29244334 -0.10499473  0.39469022 -0.8089991   0.86934066\n",
            "  0.5785795  -0.5142811  -0.5436694  -0.42741182  0.23162568 -0.25191343\n",
            " -0.47150403 -0.5589146  -0.68859303  0.810955    2.277868    0.53812003\n",
            "  1.32995    -1.0934389  -1.0493196   0.82483464 -0.34701926  0.31328374\n",
            " -0.15807578  0.7734189  -0.6061423  -1.5327737  -0.16377556 -0.29038188\n",
            " -0.01483013  0.15960999  0.34047565 -0.4342457  -1.5540739   0.6796881\n",
            "  0.3088135  -0.7179116  -0.6230494   1.1859444   1.7347418   0.81580174\n",
            "  0.6143985   0.3236534  -1.5851897  -1.3537488  -1.1745856   0.32116848\n",
            "  0.78393716 -0.06287261 -0.03717463 -0.04136372]\n"
          ]
        }
      ],
      "source": [
        "pubg_vector = word2vec.wv['pubg']\n",
        "print(f\"Vector for 'pubg':\\n{pubg_vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1efWPiArFxy"
      },
      "source": [
        "Finally, let's trasform the target into embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJb_tVLjrLG7",
        "outputId": "cccf08a1-6044-41d8-f7d0-afcf21b89055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 3 3 ... 3 3 3]\n",
            "[0 2 1 1 2 1 3 3 3 1 3 3 1 2 1 3 3 1 3 1 1 2 0 1 2 2 1 0 0 1 3 3 1 3 1 2 2\n",
            " 0 3 2 3 2 2 2 3 2 1 1 1 2 3 1 1 3 3 3 3 3 1 0 1 3 3 0 1 2 1 0 2 1 3 1 1 3\n",
            " 3 0 3 0 2 2 2 3 3 2 3 2 1 0 1 2 2 1 3 0 0 1 1 1 2 3 2 1 3 3 2 3 2 3 1 2 2\n",
            " 2 1 2 1 2 2 3 3 2 1 1 3 1 2 1 3 2 1 2 0 3 2 3 3 0 2 2 0 0 0 2 2 0 0 0 3 2\n",
            " 3 0 3 1 2 2 2 0 2 1 2 3 1 2 1 0 0 0 2 1 1 1 3 3 3 2 2 3 0 2 2 2 3 2 1 1 2\n",
            " 3 3 0 0 2 3 3 2 0 2 1 1 1 1 3 2 2 3 3 3 3 1 3 3 0 2 0 1 1 0 0 1 3 3 1 0 1\n",
            " 3 3 1 0 0 3 3 1 3 0 2 0 0 1 2 2 3 1 0 0 3 3 0 0 2 3 1 1 3 3 3 3 2 2 3 1 2\n",
            " 3 2 1 2 2 1 3 3 0 1 2 0 3 2 0 1 2 1 3 3 1 1 1 3 1 2 3 2 2 1 3 1 3 1 0 2 2\n",
            " 3 1 2 1 0 3 1 3 0 3 3 3 3 3 1 1 3 1 2 2 2 3 0 2 3 0 1 2 2 0 2 2 0 1 3 1 0\n",
            " 0 3 0 3 2 2 0 0 1 1 1 2 3 0 2 1 3 0 2 1 1 1 3 2 2 0 1 3 3 0 2 0 3 2 2 3 3\n",
            " 1 2 3 1 2 1 0 1 3 3 0 3 3 2 1 2 0 0 3 2 3 1 1 1 0 3 2 3 0 1 2 0 1 3 3 3 3\n",
            " 2 2 0 2 1 3 3 2 1 3 2 1 1 1 1 1 2 2 2 3 3 1 1 2 1 2 2 1 0 3 0 0 1 3 3 3 0\n",
            " 2 3 0 2 2 1 3 2 2 2 2 0 0 0 2 2 0 2 0 2 0 1 1 3 0 0 3 0 0 2 1 1 3 3 1 3 3\n",
            " 3 3 0 3 0 2 1 3 2 2 3 3 0 3 0 3 0 2 1 1 1 1 2 3 1 1 3 3 2 2 1 1 3 2 1 1 1\n",
            " 2 2 3 1 1 1 2 2 0 2 1 1 3 0 0 1 2 1 2 3 1 2 3 3 1 2 2 3 0 1 1 2 0 2 1 3 1\n",
            " 2 1 1 1 3 1 3 1 3 1 1 2 1 2 1 3 1 0 2 1 1 2 2 3 1 3 2 2 2 0 0 2 0 1 2 2 2\n",
            " 0 1 1 0 2 0 3 2 2 1 2 3 2 2 0 2 2 0 2 3 2 3 3 0 3 2 1 3 2 0 0 2 1 3 2 2 1\n",
            " 0 2 1 0 2 0 0 3 0 1 3 1 1 2 0 2 2 2 3 3 0 3 0 1 0 1 2 3 2 0 2 1 3 0 0 3 2\n",
            " 3 1 1 2 3 1 3 1 2 3 3 2 2 2 0 3 2 1 2 2 1 3 1 0 2 3 3 3 0 3 2 1 2 3 3 0 3\n",
            " 3 3 1 2 3 2 2 1 1 1 2 3 2 1 0 3 3 3 0 2 3 0 1 2 1 2 0 1 2 2 3 1 3 3 2 1 2\n",
            " 1 1 1 0 1 0 3 3 3 3 2 1 1 3 1 1 2 0 2 3 2 1 2 3 1 2 3 1 2 2 3 0 1 2 0 1 0\n",
            " 3 3 2 3 2 3 1 2 2 2 3 2 2 0 1 3 2 1 1 2 2 3 1 1 1 0 1 3 2 2 3 0 2 1 3 3 1\n",
            " 0 3 3 2 1 2 3 3 1 1 1 0 3 1 2 1 2 2 0 3 3 1 2 3 1 1 1 1 1 1 1 0 0 1 2 1 2\n",
            " 2 2 3 2 2 2 0 0 1 2 3 2 1 1 3 2 0 3 2 3 0 2 1 0 0 0 3 3 0 3 3 0 1 1 3 2 2\n",
            " 2 2 2 2 1 1 1 1 3 1 2 3 0 2 1 3 3 1 3 2 2 0 3 1 0 2 0 1 2 3 0 0 3 2 1 3 1\n",
            " 1 2 1 2 0 1 3 0 2 1 1 3 3 0 0 1 0 2 1 2 2 2 2 2 2 0 3 2 3 3 3 1 2 3 2 0 0\n",
            " 3 2 3 2 0 3 2 1 1 3 3 3 0 3 2 0 1 3 3 1 1 1 2 3 0 2 3 3 2 3 1 3 0 0 3 3 2]\n",
            "Label mapping: {'Irrelevant': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 2. Fit the label encoder and transform the target labels to numeric form\n",
        "y_train_encoded = label_encoder.fit_transform(train_target)\n",
        "print(y_train_encoded)\n",
        "\n",
        "y_test_encoded = label_encoder.transform(test_target)\n",
        "print(y_test_encoded)\n",
        "\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bhIotPlwbph",
        "outputId": "a0e2731d-4fc9-4b51-b5c4-4ee8b5256809"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 3, 3, ..., 3, 3, 3])"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(y_train_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Gv0WWosGBj"
      },
      "source": [
        "save 3 types of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ccxlRrv0nu"
      },
      "outputs": [],
      "source": [
        "# save y_train\n",
        "np.save('/content/drive/My Drive/y_train.npy', np.array(y_train_encoded))\n",
        "\n",
        "\n",
        "# save y_test\n",
        "np.save('/content/drive/My Drive/y_test.npy', np.array(y_test_encoded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lI3qHWHkv4fV",
        "outputId": "c76a357d-6b8f-4a71-e339-463e92f49b0c"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "17630682770 requested and 7829178352 written",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-b475ccf222b3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# save x_train_tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/x_train_tfidf.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# save x_train_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    547\u001b[0m                            pickle_kwargs=dict(fix_imports=fix_imports))\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             for chunk in numpy.nditer(\n",
            "\u001b[0;31mOSError\u001b[0m: 17630682770 requested and 7829178352 written"
          ]
        }
      ],
      "source": [
        "# save x_train_cv\n",
        "np.save('/content/drive/My Drive/x_train_cv.npy', train_cv.toarray())\n",
        "\n",
        "# save x_train_tfidf\n",
        "np.save('/content/drive/My Drive/x_train_tfidf.npy', train_tfidf.toarray())\n",
        "\n",
        "# save x_train_word2vec\n",
        "np.save('/content/drive/My Drive/x_train_word2vec.npy', train_word2vec)\n",
        "\n",
        "# save x_test_cv\n",
        "np.save('/content/drive/My Drive/x_test_cv.npy', test_cv.toarray())\n",
        "\n",
        "\n",
        "# save x_test_tfidf\n",
        "np.save('/content/drive/My Drive/x_test_tfidf.npy', test_tfidf.toarray())\n",
        "\n",
        "# save x_test_word2vec\n",
        "np.save('/content/drive/My Drive/x_test_word2vec.npy', test_word2vec)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}