{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bZWgptcSb-_",
        "outputId": "11b5abef-4541-4cba-f0fc-ba074874ae21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model explainability\n",
        "- Feature importance\n",
        "- Data bias and model bias\n",
        "- Model risk"
      ],
      "metadata": {
        "id": "n5z4IQA8dRt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xE95YfAeMU9x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "szEdtBXiFYcE"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load Word2Vec data\n",
        "X_train_w2v = joblib.load('/content/drive/MyDrive/aap/X_train_w2v.joblib')\n",
        "X_val_w2v = joblib.load('/content/drive/MyDrive/aap/X_val_w2v.joblib')\n",
        "X_test_w2v = joblib.load('/content/drive/MyDrive/aap/X_test_w2v.joblib')\n",
        "\n",
        "\n",
        "y_train_w2v = joblib.load('/content/drive/MyDrive/aap/y_train_w2v.joblib')\n",
        "y_val_w2v = joblib.load('/content/drive/MyDrive/aap/y_val_w2v.joblib')\n",
        "\n",
        "\n",
        "y_test = joblib.load('/content/drive/MyDrive/aap/y_test.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "xgb_clf = XGBClassifier(\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=400,\n",
        "        max_depth=8,\n",
        "        objective='multi:softmax',\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state = 42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "\n",
        "\n",
        "xgb_clf.fit(X_train_w2v, y_train_w2v)\n",
        "\n",
        "# Make predictions on validation and test sets\n",
        "y_val_pred = xgb_clf.predict(X_val_w2v)\n",
        "y_pred_w2v = xgb_clf.predict(X_test_w2v)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_accuracy = accuracy_score(y_val_w2v, y_val_pred)\n",
        "# Print the validation accuracy\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "accuracy_cv = accuracy_score(y_test, y_pred_w2v)\n",
        "report_cv = classification_report(y_test, y_pred_w2v)\n",
        "\n",
        "print(f\"Test Accuracy for word2vec: {accuracy_cv * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fwYlGI9L-ax",
        "outputId": "1f78fcce-d07d-4d14-9b4d-37532f58703e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:00:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 76.66%\n",
            "Test Accuracy for word2vec: 90.20%\n",
            "CPU times: user 7min 24s, sys: 1.51 s, total: 7min 26s\n",
            "Wall time: 58.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wOfCOAW4FC2",
        "outputId": "52a3dc3a-fee9-4f2d-d4df-543714288955"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/My Drive/aap/train_clean.csv'\n",
        "test_path = '/content/drive/My Drive/aap/test_clean.csv'\n",
        "train_data = pd.read_csv(train_path,sep = \",\", header=0)\n",
        "test_data = pd.read_csv(test_path, sep = \",\", header=0)"
      ],
      "metadata": {
        "id": "_e-aaXrk6uVW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_data[\"preprocessed_text\"]\n",
        "train_target = train_data[\"type\"]\n",
        "\n",
        "test = test_data[\"preprocessed_text\"]\n",
        "test_target = test_data[\"type\"]\n",
        "\n",
        "# Create a new DataFrame for the train data with preprocessed text and target\n",
        "train_combined = pd.DataFrame({\n",
        "    'preprocessed_text': train_data[\"preprocessed_text\"],\n",
        "    'target': train_data[\"type\"]\n",
        "})\n",
        "\n",
        "# Create a new DataFrame for the test data with preprocessed text and target\n",
        "test_combined = pd.DataFrame({\n",
        "    'preprocessed_text': test_data[\"preprocessed_text\"],\n",
        "    'target': test_data[\"type\"]\n",
        "})"
      ],
      "metadata": {
        "id": "6hfcf_5U68mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with missing data and show the missing data rate\n",
        "\n",
        "def remove_missing_data_and_calculate_rate(df):\n",
        "    # Step 1: Calculate the missing data rate\n",
        "    missing_data_rate = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "    # Step 2: Drop rows with missing data\n",
        "    df_cleaned = df.dropna()\n",
        "\n",
        "    return df_cleaned, missing_data_rate\n",
        "\n",
        "train_cleaned, missing_data_rate = remove_missing_data_and_calculate_rate(train_combined)\n"
      ],
      "metadata": {
        "id": "GNDrvF4D68pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train Word2Vec and transform data into embeddings\n",
        "\n",
        "def get_sentence_vector(sentence, model, vector_size):\n",
        "    sentence_vector = np.zeros(vector_size)  # Initialize an empty vector\n",
        "    count = 0\n",
        "    for word in sentence.split():\n",
        "        if word in model.wv:\n",
        "            sentence_vector += model.wv[word]\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        sentence_vector /= count  # Average the vectors\n",
        "    return sentence_vector\n",
        "\n",
        "def word2vec(train, test=None):\n",
        "    # Step 1: Train Word2Vec model on training data\n",
        "    word2vec_model = Word2Vec(sentences=[i.split() for i in train], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "    # Step 2: Function to generate sentence vectors by averaging word vectors\n",
        "\n",
        "    # Step 3: Convert train dataset to sentence vectors\n",
        "    train_vectors = np.array([get_sentence_vector(sentence, word2vec_model, 100) for sentence in train])\n",
        "\n",
        "    # If test data is provided, apply the same transformation\n",
        "    if test is not None:\n",
        "        test_vectors = np.array([get_sentence_vector(sentence, word2vec_model, 100) for sentence in test])\n",
        "        return train_vectors, test_vectors, word2vec_model\n",
        "    else:\n",
        "        return train_vectors, word2vec_model\n"
      ],
      "metadata": {
        "id": "vrdlsfKB7GdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_word2vec, test_word2vec, word2vec_model = word2vec(train, test)"
      ],
      "metadata": {
        "id": "JtEPDiJh7acf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the model"
      ],
      "metadata": {
        "id": "JmuEAVaB37_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save the trained model\n",
        "model_path = '/content/drive/MyDrive/aap/xgb_w2v_model.joblib'\n",
        "joblib.dump(xgb_clf, model_path)\n",
        "print(f\"Model saved at {model_path}\")\n"
      ],
      "metadata": {
        "id": "4Yna9cQGrCDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25998ef8-9d76-4c55-ff93-10c5556c4ce6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at /content/drive/MyDrive/aap/xgb_w2v_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the embedding model"
      ],
      "metadata": {
        "id": "ISlQBd8F4Ctn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained Word2Vec model\n",
        "word2vec_model.save('/content/drive/MyDrive/aap/word2vec_model.model')\n"
      ],
      "metadata": {
        "id": "HeXBnEwM7wvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for future use"
      ],
      "metadata": {
        "id": "rkoTN9zu73T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New test string or list of strings\n",
        "new_test_data = [\"This is a new test sentence\", \"Another example sentence for testing\"]\n",
        "\n",
        "# Use the get_sentence_vector function to convert the new data to vectors\n",
        "def transform_new_data(new_data, model, vector_size):\n",
        "    \"\"\"\n",
        "    Transform new text data into Word2Vec sentence vectors using the trained model.\n",
        "    Args:\n",
        "        new_data (list of str): List of sentences to be transformed.\n",
        "        model (Word2Vec): Trained Word2Vec model.\n",
        "        vector_size (int): Dimension of the Word2Vec embeddings.\n",
        "    Returns:\n",
        "        numpy.ndarray: Transformed sentence vectors.\n",
        "    \"\"\"\n",
        "    return np.array([get_sentence_vector(sentence, model, vector_size) for sentence in new_data])\n",
        "\n",
        "# Convert new test data to vectors\n"
      ],
      "metadata": {
        "id": "2Sni2p4771DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load the saved Word2Vec model\n",
        "loaded_model = Word2Vec.load('word2vec_model.model')  # or 'word2vec_model_binary.model' for binary\n",
        "model_path = '/content/drive/MyDrive/aap/xgb_w2v_model.joblib'\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "xgb_clf = joblib.load(model_path)\n",
        "\n",
        "# Now you can use the model to make predictions or further evaluate it\n"
      ],
      "metadata": {
        "id": "GsTZrwqu71JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_data = [\"......anything you like\"]"
      ],
      "metadata": {
        "id": "Ndm5rdHA9Uaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_vectors = transform_new_data(new_test_data, loaded_model, 100)"
      ],
      "metadata": {
        "id": "v5qMRkJ28VtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_w2v = xgb_clf.predict(new_test_vectors)"
      ],
      "metadata": {
        "id": "6xw9ePmw8sM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}